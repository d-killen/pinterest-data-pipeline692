{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "482f8510-27e6-4ebe-b16c-dc07cf513358",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Databrick Stream Processing Notebook\n",
    "This notebook was originally written in Databricks to the perform the following tasks:\n",
    "    1. Connect to AWS and retrieve the data\n",
    "    2. Clean the data\n",
    "    3. Build the tables\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c123f23-e1ce-4d28-ae3a-e2574527164d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Retrieve Access Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad4acac6-4fd8-4b7f-83c9-a9f97154ac22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import statements\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col,when\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "import urllib\n",
    "\n",
    "# retrieve amazon credentials\n",
    "# specify file type to be csv\n",
    "file_type = \"csv\"\n",
    "\n",
    "# indicates file has first row as the header\n",
    "first_row_is_header = \"true\"\n",
    "\n",
    "# indicates file has comma as the delimeter\n",
    "delimiter = \",\"\n",
    "\n",
    "# read the CSV file to spark dataframe\n",
    "aws_keys_df = spark.read.format(file_type)\\\n",
    ".option(\"header\", first_row_is_header)\\\n",
    ".option(\"sep\", delimiter)\\\n",
    ".load(\"/FileStore/tables/authentication_credentials.csv\")\n",
    "\n",
    "# get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "\n",
    "# encode the secret key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb675853-655c-40b1-bd38-ec557de7ffb7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Build the Pin Dataframe and Table\n",
    "\n",
    "This section creates and cleans the dataframe containing the pin data and loads to a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f76e1f3c-3678-4118-adc5-9b2dcfdd3715",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create the dataframe\n",
    "df_pin = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-0a25072a5e0f-pin') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "\n",
    "# specify schema\n",
    "pin_schema = StructType([\n",
    "    StructField(\"index\", IntegerType()),\n",
    "    StructField(\"unique_id\", StringType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"description\", StringType()),\n",
    "    StructField(\"poster_name\", StringType()),\n",
    "    StructField(\"follower_count\", StringType()),\n",
    "    StructField(\"tag_list\", StringType()),\n",
    "    StructField(\"is_image_or_video\", StringType()),\n",
    "    StructField(\"image_src\", StringType()),\n",
    "    StructField(\"downloaded\", IntegerType()),\n",
    "    StructField(\"save_location\", StringType()),\n",
    "    StructField(\"category\", StringType())\n",
    "])\n",
    "\n",
    "# deserialize\n",
    "deserialize_df_pin = df_pin.selectExpr(\"CAST(data as STRING)\")\n",
    "deserialize_df_pin = deserialize_df_pin.withColumn(\"data\", from_json(col(\"data\"), pin_schema))\n",
    "deserialize_df_pin = deserialize_df_pin.selectExpr(\"data.*\")\n",
    "\n",
    "# remove duplicates\n",
    "df_pin = deserialize_df_pin.drop_duplicates([column_name for column_name, data_type in deserialize_df_pin.dtypes])\n",
    "\n",
    "# replace null values into None\n",
    "df_pin_clean = df_pin.withColumn(\"category\", when(df_pin[\"category\"].isNull(), None).otherwise(df_pin[\"category\"]))\n",
    "df_pin_clean = df_pin.withColumn(\"description\", when(df_pin[\"description\"].isNull(), None).otherwise(df_pin[\"description\"]))\n",
    "\n",
    "# replace entries with no relevant data with None\n",
    "df_pin_clean = df_pin.withColumn(\"description\", when(col(\"description\").contains(\"No description\"), None).otherwise(col(\"description\")))\n",
    "df_pin_clean = df_pin.withColumn(\"follower_count\", when(col(\"follower_count\").contains(\"User Info Error\"), None).otherwise(col(\"follower_count\")))\n",
    "df_pin_clean = df_pin.withColumn(\"image_src\", when(col(\"image_src\").contains(\"Image src error\"), None).otherwise(col(\"image_src\")))\n",
    "df_pin_clean = df_pin.withColumn(\"poster_name\", when(col(\"poster_name\").contains(\"User Info Error\"), None).otherwise(col(\"poster_name\")))\n",
    "\n",
    "# change M and k inside follower_column into its coresponding value\n",
    "df_pin_clean = df_pin_clean.withColumn(\"follower_count\", regexp_replace(df_pin_clean[\"follower_count\"], \"M\", \"000000\"))\n",
    "df_pin_clean = df_pin_clean.withColumn(\"follower_count\", regexp_replace(df_pin_clean[\"follower_count\"], \"k\", \"000\"))\n",
    "\n",
    "# change follower_count data type into int\n",
    "df_pin_clean = df_pin_clean.withColumn(\"follower_count\", df_pin_clean[\"follower_count\"].cast(\"int\"))\n",
    "\n",
    "# ensuring that each column containing numeric data has a numeric data type\n",
    "df_pin_clean = df_pin_clean.withColumn(\"downloaded\", df_pin_clean[\"downloaded\"].cast(\"int\"))\n",
    "df_pin_clean = df_pin_clean.withColumn(\"index\", df_pin_clean[\"index\"].cast(\"int\"))\n",
    "\n",
    "# cleaning the save_location column\n",
    "df_pin_clean = df_pin_clean.withColumn(\"save_location\", regexp_replace(df_pin_clean[\"save_location\"], \"Local save in\", \"\"))\n",
    "\n",
    "# renaming index column into ind column\n",
    "df_pin_clean = df_pin_clean.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "# reorder dataframe columns\n",
    "df_pin_clean = df_pin_clean.select([\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\"])\n",
    "\n",
    "try:\n",
    "  # deletes the checkpoint folder\n",
    "  dbutils.fs.rm(\"/tmp/kinesis/_checkpoints/pin/\", True)\n",
    "except:\n",
    "  print(\"There is no saved checkpoints folder.\")\n",
    "\n",
    "# writes df_pin_clean dataframe into deltatables\n",
    "df_pin_clean.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/pin/\") \\\n",
    "  .table(\"0a25072a5e0f_pin_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b77f3216-c63f-4f72-aeca-5cbd41e47236",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Build the Geo Dataframe and Table\n",
    "\n",
    "This section creates and cleans the dataframe containing the geo data and loads to a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24cb03f3-2c1e-4357-b822-9bffebe4a422",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create the dataframe\n",
    "df_geo = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-0a25072a5e0f-geo') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "\n",
    "# specify schema\n",
    "geo_schema = StructType([\n",
    "    StructField(\"ind\", IntegerType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"latitude\", DoubleType()),\n",
    "    StructField(\"longitude\", DoubleType()),\n",
    "    StructField(\"country\", StringType())\n",
    "])\n",
    "\n",
    "# deserialize\n",
    "deserialize_df_geo = df_geo.selectExpr(\"CAST(data as STRING)\")\n",
    "deserialize_df_geo = deserialize_df_geo.withColumn(\"data\", from_json(col(\"data\"), geo_schema))\n",
    "deserialize_df_geo = deserialize_df_geo.selectExpr(\"data.*\")\n",
    "\n",
    "# remove duplicates\n",
    "df_geo = deserialize_df_geo.drop_duplicates([column_name for column_name, data_type in deserialize_df_geo.dtypes])\n",
    "\n",
    "# creating a new coordinates column\n",
    "df_geo_clean = df_geo.withColumn(\"coordinates\", array(col(\"latitude\"), col(\"longitude\")))\n",
    "\n",
    "# dropping latitude and longitude column\n",
    "df_geo_clean = df_geo_clean.drop(\"latitude\", \"longitude\")\n",
    "\n",
    "# converting timestamp into a timestamp data type\n",
    "df_geo_clean = df_geo_clean.withColumn(\"timestamp\", df_geo_clean[\"timestamp\"].cast(\"timestamp\"))\n",
    "\n",
    "# reordering columns\n",
    "df_geo_clean = df_geo_clean.select([\"ind\", \"country\", \"coordinates\", \"timestamp\"])\n",
    "\n",
    "try:\n",
    "  # deletes the checkpoint folder\n",
    "  dbutils.fs.rm(\"/tmp/kinesis/_checkpoints/geo/\", True)\n",
    "except:\n",
    "  print(\"There is no saved checkpoints folder.\")\n",
    "\n",
    "# writes df_geo_clean dataframe into deltatables\n",
    "df_geo_clean.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/geo/\") \\\n",
    "  .table(\"0a25072a5e0f_geo_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eced2bd1-413f-4126-9b44-9dd2defcbc2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## Build the User Dataframe and Table\n",
    "\n",
    "This section creates and cleans the dataframe containing the user data and loads to a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4557fd84-9d18-4b17-91f2-65474cd042dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create the dataframe\n",
    "df_user = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-0a25072a5e0f-user') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "\n",
    "# specify schema\n",
    "user_schema = StructType([\n",
    "    StructField(\"ind\", IntegerType()),\n",
    "    StructField(\"first_name\", StringType()),\n",
    "    StructField(\"last_name\", StringType()),\n",
    "    StructField(\"age\", StringType()),\n",
    "    StructField(\"date_joined\", TimestampType())\n",
    "])\n",
    "\n",
    "# deserialize\n",
    "deserialize_df_user = df_user.selectExpr(\"CAST(data as STRING)\")\n",
    "deserialize_df_user = deserialize_df_user.withColumn(\"data\", from_json(col(\"data\"), user_schema))\n",
    "deserialize_df_user = deserialize_df_user.selectExpr(\"data.*\")\n",
    "\n",
    "# remove duplicates\n",
    "df_user = deserialize_df_user.drop_duplicates([column_name for column_name, data_type in deserialize_df_user.dtypes])\n",
    "\n",
    "# creating an username column\n",
    "df_user_clean = df_user.withColumn(\"user_name\", concat(col(\"first_name\"), col(\"last_name\")))\n",
    "\n",
    "# dropping first and last name columns\n",
    "df_user_clean = df_user_clean.drop(\"first_name\", \"last_name\")\n",
    "\n",
    "# converting date_joined column into a timestamp\n",
    "df_user_clean = df_user_clean.withColumn(\"date_joined\", df_user_clean[\"date_joined\"].cast(\"timestamp\"))\n",
    "\n",
    "# reodering columns\n",
    "df_user_clean = df_user_clean.select([\"ind\",\"user_name\",\"age\",\"date_joined\"])\n",
    "\n",
    "try:\n",
    "  # deletes the checkpoint folder\n",
    "  dbutils.fs.rm(\"/tmp/kinesis/_checkpoints/user/\", True)\n",
    "except:\n",
    "  print(\"There is no saved checkpoints folder.\")\n",
    "\n",
    "# writes df_user_clean dataframe into deltatables\n",
    "df_user_clean.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/user/\") \\\n",
    "  .table(\"0a25072a5e0f_user_table\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "stream_processing_notebook",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
